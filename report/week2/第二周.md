# 探寻不同宏观情境下的策略配置规律

> 本周我们就我们这次研究的题目以及步骤进行细致的探讨，并对步骤三中的Transformer具体实现进行了理解和注释

## 题目理解

* 步骤4 线性检验

  这个划分的阈值确定起来的结果能力较差，并且这一步中宏观因子对策略的影响并不能区分是正面还是负面影响，划分因素的选择较差，因此这种线性划分的方式，在本题目中并不适合

* 步骤5 非线性建模

  首先需要通过自监督学习来学习embedding表示，由于数据还未拿到，因此这一步可能是接下来的重点（如何将每个月截面上的高维数据转变为一个token，然后用之前的token来预测未来的token），由于要屏蔽未来的数据，因此这一步可能可以使用Transformer中的decoder模块来进行实现，我们之后也会尝试使用开源的模型如GPT-2来进行尝试

  其次学习每个宏观数据对于各个策略的好坏，每个策略只有好和坏两种，因此对于每一个策略来说均为二分类问题，这里可以使用BERT这种预训练语言模型来完成，对BERT模型进行微调即可实现词分类任务
  
* 步骤6 构建母策略

  `每个月选取该宏观情境下最适合的前五个子策略`这一步就是使用第五步中最终生成每个策略好坏的一个列表（其中存放每个策略是好策略的概率，类似logistic模型中的sigmod函数），选取概率值最大的前五个策略，作为该月的子策略，以此构成母策略。

  
  
  
  
  



## Transformer实现注释

我们就这Transformer实现代码进行了详细的理解和注释，对于其本质有了更深层次的理解，下面就几个重要的板块进行代码的解读

### 多头注意力

$$
\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d_{\tilde{k}}}}\right) V
$$

```python
class MultiHeadAttention(nn.Module):
    ''' Multi-Head Attention module '''

    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):

        super().__init__()

        self.n_head = n_head #头数
        self.d_k = d_k #key和query的向量维度
        self.d_v = d_v #value的向量维度

        # 三个线性层做矩阵乘法生成q, k, v
        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False) 
        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)
        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)
        self.fc = nn.Linear(n_head * d_v, d_model, bias=False) # 将多个头学习到的向量通过一个线性层，实现多头学习到内容的拼接

        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5) #注意力计算

        self.dropout = nn.Dropout(dropout) # 丢弃率
        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6) # 这里使用layernorm的正则化方式，对比batch不容易受到时序数据长度的影响


    def forward(self, q, k, v, mask=None):

        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head 
        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1) 
        # sz_b指的是batchsize，len_q, len_k, len_v指的是三者的第二维大小，也就是每一个样本的长度（seq序列长度

        residual = q

        # Pass through the pre-attention projection: b x lq x (n*dv)
        # Separate different heads: b x lq x n x dv
        #b: batch_size, lq: translation task的seq长度, n: head数, dv: embedding vector length
        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)
        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)
        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)

        # Transpose for attention dot product: b x n x lq x dv
        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)

        if mask is not None:
            # (batchSize, 1, seqLen) -> (batchSize, 1, 1, seqLen)
            mask = mask.unsqueeze(1)   # For head axis broadcasting.

        q, attn = self.attention(q, k, v, mask=mask) # b x n x lq x dv

        # Transpose to move the head dimension back: b x lq x n x dv
        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)
        # 将多个头学习到的连在一起
        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1) # .contiguous()：这是为了保证内存中的连续性 
        
        # 将连接的结果再进入一层线性层进行投影 其实这这个线性层的输入和输出维度在论文中是一致的，
        #但是其实在实际操作中可以不一致，该层也可以将最终的维度确定在d_model的大小，论文中是512
        q = self.dropout(self.fc(q)) 

        q += residual #残差连接
        q = self.layer_norm(q) #进行norm操作

        return q, attn
class ScaledDotProductAttention(nn.Module):
    ''' Scaled Dot-Product Attention '''
    # 实现注意力机制的点积算法

    def __init__(self, temperature, attn_dropout=0.1):
        super().__init__()
        self.temperature = temperature #sqrt（dk） 即为对qk的乘积进行scale操作
        self.dropout = nn.Dropout(attn_dropout)

    def forward(self, q, k, v, mask=None):

        attn = torch.matmul(q / self.temperature, k.transpose(2, 3)) #transpose函数将k的2,3维进行了交换  生成权重向量
        # b x n x lq x dv @ b x n x dv x lq = b x n x lq x lq

        if mask is not None:
            attn = attn.masked_fill(mask == 0, -1e9) #若是decoder中的带掩码的自注意力机制或者是存在无意义的占位符，则需要进行mask操作

        attn = self.dropout(F.softmax(attn, dim=-1)) #Dropout 操作会随机将一些元素置零，以减小网络对某些特定输入的依赖性，从而提高模型的泛化能力
        output = torch.matmul(attn, v) #类似于对v向量进行加权和  b x n x lq x dv

        return output, attn #返回输出向量和权重向量
```

### 掩码

在Transformer中两种掩码： 

**Padding Mask**

因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的Attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。

具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过softmax，这些位置的概率就会接近0！ 而我们的padding mask 实际上是一个张量，每个值都是一个Boolean，值为false的地方就是我们要进行处理的地方。

**Sequence mask**

sequence mask是为了使得Decoder不能看见未来的信息。也就是对于一个序列，在time_step为t的时刻，我们的解码输出应该只能依赖于t时刻之前的输出，而不能依赖t之后的输出。因此我们需要想一个办法，把t之后的信息给隐藏起来。 那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。

```python
def get_pad_mask(seq, pad_idx):
    # (batch, seqlen) -> (batch, 1, seqlen) 
    return (seq != pad_idx).unsqueeze(-2)


def get_subsequent_mask(seq):
    ''' For masking out the subsequent info. '''
    sz_b, len_s = seq.size()
    # torch.triu(diagonal=1)保留矩阵上三角部分，其余部分(包括对角线)定义为0。
    subsequent_mask = (1 - torch.triu(
        torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()
    return subsequent_mask

```

### 位置编码

$$
\begin{aligned}
P E_{(\text {pos }, 2 i)} & =\sin \left(\frac{p o s}{10000^{2 i / d_{\text {model }}}}\right) \\
P E_{(\text {pos }, 2 i+1)} & =\cos \left(\frac{p o s}{10000^{2 i / d_{\text {model }}}}\right)
\end{aligned}
$$

```python
class PositionalEncoding(nn.Module):
    ''' 位置编码
    '''
    def __init__(self, d_hid, n_position=200):
        super(PositionalEncoding, self).__init__()

        # Not a parameter
        # 将tensor注册成buffer, optim.step()的时候不会更新
        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_position, d_hid)) # 在模型的训练过程中不会被更新

    def _get_sinusoid_encoding_table(self, n_position, d_hid):
        ''' Sinusoid position encoding table '''

        def get_position_angle_vec(position):
            # 2i, 所以此处要整除2
            return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]

        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])
        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i 偶数项
        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1 奇数项

        return torch.FloatTensor(sinusoid_table).unsqueeze(0) # shape:(1, maxLen(n_position), d_hid)

    def forward(self, x):
        return x + self.pos_table[:, :x.size(1)].clone().detach() #该操作保证梯度不被修改，这里使用了张量的广播机制
```

