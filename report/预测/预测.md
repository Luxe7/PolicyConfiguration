# 预测策略

## 训练设计

在训练的设计阶段，考虑使用预训练模型BERT进行微调，但是考虑到输入的数据并不是时序的信息，使用基于注意力机制的bert模型可能在此处的效果并不是很好，因此并没有考虑使用使用预训练语言模型进行本次的分类任务，选择了使用深度学习中的`多层感知机MLP`进行了分类任务

## 多层感知机

多层感知机（MLP，Multi-Layer Perceptron）是一种前馈神经网络，由一个输入层、一个或多个隐藏层和一个输出层组成。每一层都由多个神经元组成，层与层之间通过全连接的方式相连。通过引入非线性的激活函数（本项目中使用了`relu函数`），多层感知机拥有了拟合任何复杂函数的能力，在这里可以实现分类任务的任务需求。

![image-20240301114258171](https://typora-ljy.oss-cn-shanghai.aliyuncs.com/image-20240301114258171.png)

在本项目中，由于在第一步中已经进行了embedding的预训练，因此可以假定训练的嵌入向量已经可以拥有预测未来信息的能力，在分类时使用一个MLP就可以达到比较好的效果，用来进行回测。

构造的MLP模型如下：

```python
# 分类模型
class MLPRegressor(nn.Module):
    def __init__(self, input_size=train_X.size(1), hidden_size=256,output_size=train_Y.size(1)):
        super(MLPRegressor, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)  # Output is a single regression value
        self.sigmod=nn.Sigmoid()

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return self.sigmod(x)
```

## 实验设计

* `策略指数数据爬取`：通过uqer平台，找到2007年-2023年所有的策略指数
* `Y值选取（标签设定）`：首先对每一个策略指数在每一个月的净值进行判断在后一个月是否会产生明显的超额收益，通过判断净值的增长率，选出最大的五个策略，将它们设置为1，认为它们是具有明显的超额收益的策略，其余的设置为0，认为没有超额收益。
* `模型选择`：使用了MLP模型进行分类任务
* `损失函数`：对于本次任务来说，并不是多分类任务（`softmax函数+交叉熵损失函数`），而是打标签的任务，每一个策略函数的值需要通过`Sigmod`函数进行概率预测，生成该策略会产生超额收益的概率，对每一个策略使用`BCE损失函数`，即可使用反向传播的算法进行参数更新，使模型训练效果更好。
* `预测任务`：将所有的数据（宏观数据的embedding）放入已经训练好的模型中，获取到所有策略会产生超额收益的概率，选取其中最大的五个策略，将它们预测为产生超额收益的策略，生成标签1，其余设置为0

## 实验难点

在训练的过程中出现了损失值始终不变，无论进行多少轮的迭代，都会出现这个情况，经过对超参数的分析和尝试后得到可能是学习率设置的过大为0.1，出现了步长过大的问题，始终无法找到最小值，因此损失很大。在将步长改变为1.5e-4、并使用adam优化器时，损失出现了下降，模型逐渐收敛。

## 实验结果

模型在训练之后，对于测试集的预测正确率为`0.75`，效果较好，接下来会将预测的结果进行回测，进一步衡量其效果

具体代码可见`Finding_goodPolicy.ipynb`文件